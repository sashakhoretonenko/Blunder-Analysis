{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Go to parent directory\n",
    "from generating_data.generate_loaders import (\n",
    "    create_Tang_multi_loaders,\n",
    "    create_GM_test_multi_loaders\n",
    "    )\n",
    "from models.CNNs import (multiTrainer, CNN_1convlayer_k3_1afflayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tang_train_loader, Tang_val_loader, Tang_test_loader = create_Tang_multi_loaders('../data/pkl/moves/Tang_moves.pkl')\n",
    "model = CNN_1convlayer_k3_1afflayer()\n",
    "trainer = multiTrainer(net=model, train_loader=Tang_train_loader, val_loader=Tang_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1186/1186 [00:37<00:00, 31.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss = 1.4683, Train Acc = 0.3781, Val Loss = 1.3715, Val Acc = 0.4194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1186/1186 [00:34<00:00, 34.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Loss = 1.3439, Train Acc = 0.4307, Val Loss = 1.3257, Val Acc = 0.4383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 496/1186 [00:14<00:19, 34.64it/s]"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "completed_model, best_model, train_losses, val_losses, train_acc, val_acc = trainer.train(\n",
    "    optim_name='adamw',\n",
    "    loss_function=nn.CrossEntropyLoss(),\n",
    "    num_epochs=10,\n",
    "    learning_rate=0.001,\n",
    "    momentum=0.9,\n",
    "    step_size=5,\n",
    "    learning_rate_decay=0.9,\n",
    "    acc_frequency=1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_per_piece_accuracy(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    class_correct = [0] * 6\n",
    "    class_total = [0] * 6\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    classes = ['P', 'N', 'B', 'R', 'Q', 'K']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm.tqdm(test_loader):\n",
    "            X, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Collect all labels and predictions for metrics\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Calculate per-class accuracy\n",
    "            for i in range(len(y)):\n",
    "                label = y[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == y[i]:\n",
    "                    class_correct[label] += 1\n",
    "\n",
    "    # Calculate metrics for each piece\n",
    "    correct_counts = []\n",
    "    incorrect_counts = []\n",
    "    accuracies = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(6):\n",
    "        if class_total[i] > 0:\n",
    "            accuracy = 100 * class_correct[i] / class_total[i]\n",
    "            recall = recall_score(all_labels, all_preds, labels=[i], average='macro')\n",
    "            f1 = f1_score(all_labels, all_preds, labels=[i], average='macro')\n",
    "            correct_counts.append(class_correct[i])\n",
    "            incorrect_counts.append(class_total[i] - class_correct[i])\n",
    "            accuracies.append(accuracy)\n",
    "            recalls.append(recall * 100)\n",
    "            f1_scores.append(f1 * 100)\n",
    "            print(f\"{classes[i]}: Accuracy: {accuracy:.2f}%\")\n",
    "        else:\n",
    "            print(f\"{classes[i]}: No samples available.\")\n",
    "            correct_counts.append(0)\n",
    "            incorrect_counts.append(0)\n",
    "            accuracies.append(0)\n",
    "            recalls.append(0)\n",
    "            f1_scores.append(0)\n",
    "\n",
    "    # ✅ Plotting the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bar_width = 0.35\n",
    "    index = range(len(classes))\n",
    "\n",
    "    plt.bar(index, correct_counts, bar_width, label='Correct')\n",
    "    plt.bar([i + bar_width for i in index], incorrect_counts, bar_width, label='Incorrect')\n",
    "\n",
    "    plt.xlabel('Piece Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Correct vs Incorrect Predictions by Piece Type')\n",
    "    plt.xticks([i + bar_width / 2 for i in index], classes)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ✅ Create a DataFrame for a summary\n",
    "    df = pd.DataFrame({\n",
    "        'Piece Type': classes,\n",
    "        'Accuracy (%)': accuracies,\n",
    "        'Recall (%)': recalls,\n",
    "        'F1 Score (%)': f1_scores\n",
    "    })\n",
    "\n",
    "    # ✅ Calculate weighted metrics\n",
    "    weighted_accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    weighted_recall = recall_score(all_labels, all_preds, average='weighted') * 100\n",
    "    weighted_f1 = f1_score(all_labels, all_preds, average='weighted') * 100\n",
    "\n",
    "    # ✅ Add weighted metrics to the DataFrame\n",
    "    df.loc['Weighted'] = ['Weighted', weighted_accuracy, weighted_recall, weighted_f1]\n",
    "\n",
    "    print(\"\\nOverall Accuracy Metrics:\")\n",
    "    print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "df_metrics = evaluate_per_piece_accuracy(best_model, Tang_test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cos333",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
